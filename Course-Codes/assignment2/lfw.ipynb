{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n",
      "Total dataset size:\n",
      "n_samples: 639\n",
      "n_features: 1850\n",
      "n_classes: 2\n",
      "Projecting the input data on the eigenfaces orthonormal basis\n",
      "done in 2.374s\n",
      "Fitting the classifier to the training set\n",
      "done in 3.145s\n",
      "Best estimator found by grid search:\n",
      "SVC(C=1000.0, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "Predicting people's names on the test set\n",
      "done in 0.006s\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "    George W Bush       0.98      0.98      0.98       138\n",
      "Gerhard Schroeder       0.86      0.86      0.86        22\n",
      "\n",
      "      avg / total       0.96      0.96      0.96       160\n",
      "\n",
      "[[135   3]\n",
      " [  3  19]]\n"
     ]
    }
   ],
   "source": [
    "##this is the code for the T1 1)\n",
    "from time import time\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "\n",
    "def meanX(dataX):\n",
    "    return np.mean(dataX,axis=0)\n",
    "\n",
    "def my_pca_FV(X_train, X_test):\n",
    "    average = meanX(X_train) \n",
    "    m, n = np.shape(X_train)\n",
    "    data_adjust = []\n",
    "    avgs = np.tile(average, (m, 1))\n",
    "    data_adjust = X_train - avgs\n",
    "    covX = np.cov(data_adjust.T)   #计算协方差矩阵\n",
    "    featValue, featVec=  np.linalg.eig(covX)  #求解协方差矩阵的特征值和特征向量\n",
    "    index = np.argsort(-featValue) #依照featValue进行从大到小排序\n",
    "    finalData = []\n",
    "    selectVec = np.matrix(featVec.T[index[:n_components]]) #这里要转置，和下面SVD对比\n",
    "    X_train_pca = -(data_adjust * selectVec.T).real/700\n",
    "\n",
    "    average = meanX(X_test) \n",
    "    m, n = np.shape(X_test)\n",
    "    data_adjust = []\n",
    "    avgs = np.tile(average, (m, 1))\n",
    "    data_adjust = X_test - avgs\n",
    "    X_test_pca = -(data_adjust * selectVec.T).real/700\n",
    "    return X_train_pca, X_test_pca\n",
    "\n",
    "def my_pca_SVD(X_train, X_test):\n",
    "    X_train_mypca=[]\n",
    "    X_test_mypca=[]\n",
    "    average = meanX(X_train) \n",
    "    m, n = np.shape(X_train)\n",
    "    data_adjust = []\n",
    "    avgs = np.tile(average, (m, 1))\n",
    "    data_adjust = X_train - avgs\n",
    "    u,v,w=np.linalg.svd(data_adjust)\n",
    "    featValue, featVec=  np.linalg.eig(covX)  #求解协方差矩阵的特征值和特征向量\n",
    "    index = np.argsort(-v) #依照featValue进行从大到小排序\n",
    "    selectVec = np.matrix(w[index[:n_components]]) #注意这个地方不需要转置！\n",
    "    X_train_mypca = -(data_adjust * selectVec.T).real/700\n",
    "\n",
    "    average = meanX(X_test) \n",
    "    m, n = np.shape(X_test)\n",
    "    data_adjust = []\n",
    "    avgs = np.tile(average, (m, 1))\n",
    "    data_adjust = X_test - avgs\n",
    "    X_test_mypca = -(data_adjust * selectVec.T).real/700\n",
    "    return X_train_mypca, X_test_mypca\n",
    "\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "# Display progress logs on stdout\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(message)s')\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "# Download the data, if not already on disk and load it as numpy arrays\n",
    "\n",
    "lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)\n",
    "\n",
    "# introspect the images arrays to find the shapes (for plotting)\n",
    "n_samples, h, w = lfw_people.images.shape\n",
    "\n",
    "# for machine learning we use the 2 data directly (as relative pixel\n",
    "# positions info is ignored by this model)\n",
    "X = lfw_people.data\n",
    "n_features = X.shape[1]\n",
    "\n",
    "# the label to predict is the id of the person\n",
    "y = lfw_people.target\n",
    "target_names = lfw_people.target_names\n",
    "n_classes = target_names.shape[0]\n",
    "\n",
    "print(\"Total dataset size:\")\n",
    "print(\"n_samples: %d\" % n_samples)\n",
    "print(\"n_features: %d\" % n_features)\n",
    "print(\"n_classes: %d\" % n_classes)\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "# Split into a training set and a test set using a stratified k fold\n",
    "\n",
    "# split into a training and testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "# Compute a PCA (eigenfaces) on the face dataset (treated as unlabeled\n",
    "# dataset): unsupervised feature extraction / dimensionality reduction\n",
    "n_components = 150\n",
    "print(\"Projecting the input data on the eigenfaces orthonormal basis\")\n",
    "t0 = time()\n",
    "X_train_pca,X_test_pca=my_pca_FV(X_train,X_test)\n",
    "#X_train_pca,X_test_pca=my_pca_SVD(X_train,X_test)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "# Train a SVM classification model\n",
    "\n",
    "print(\"Fitting the classifier to the training set\")\n",
    "t0 = time()\n",
    "param_grid = {'C': [1e3, 5e3, 1e4, 5e4, 1e5],\n",
    "              'gamma': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1], }\n",
    "clf = GridSearchCV(SVC(kernel='rbf', class_weight='balanced'),\n",
    "                   param_grid, cv=5, iid=False)\n",
    "clf = clf.fit(X_train_pca, y_train)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print(\"Best estimator found by grid search:\")\n",
    "print(clf.best_estimator_)\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "# Quantitative evaluation of the model quality on the test set\n",
    "\n",
    "print(\"Predicting people's names on the test set\")\n",
    "t0 = time()\n",
    "y_pred = clf.predict(X_test_pca)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "print(confusion_matrix(y_test, y_pred, labels=range(n_classes)))\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "# Qualitative evaluation of the predictions using matplotlib\n",
    "\n",
    "def plot_gallery(images, titles, h, w, n_row=3, n_col=4):\n",
    "    \"\"\"Helper function to plot a gallery of portraits\"\"\"\n",
    "    plt.figure(figsize=(1.8 * n_col, 2.4 * n_row))\n",
    "    plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)\n",
    "    for i in range(n_row * n_col):\n",
    "        plt.subplot(n_row, n_col, i + 1)\n",
    "        plt.imshow(images[i].reshape((h, w)), cmap=plt.cm.gray)\n",
    "        plt.title(titles[i], size=12)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "\n",
    "\n",
    "# plot the result of the prediction on a portion of the test set\n",
    "\n",
    "def title(y_pred, y_test, target_names, i):\n",
    "    pred_name = target_names[y_pred[i]].rsplit(' ', 1)[-1]\n",
    "    true_name = target_names[y_test[i]].rsplit(' ', 1)[-1]\n",
    "    return 'predicted: %s\\ntrue:      %s' % (pred_name, true_name)\n",
    "\n",
    "prediction_titles = [title(y_pred, y_test, target_names, i)\n",
    "                     for i in range(y_pred.shape[0])]\n",
    "\n",
    "plot_gallery(X_test, prediction_titles, h, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T1-1\n",
    "\n",
    "## 说明\n",
    "\n",
    "我直接将训练集数据和测试集数据作为函数参数，输出降维后的训练数据和测试数据，注意测试数据的降维必须使用训练数据的特征方向。另外，svd分解的w矩阵是转置过的。\n",
    "\n",
    "我将特征值法的函数命名为my_pca_FV，SVD方法命名为my_pca_SVD，分别调用两个函数记录运行时间。\n",
    "\n",
    "## 结果\n",
    "\n",
    "特征值法2.455秒，SVD法2.347秒，我没有使用任何优化方法，与sklearn（0.005s）相比还是非常慢，不过直接SVD分解和直接求协方差矩阵的特征值所用时间似乎差不多。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n",
      "Extracting the top 150 eigenfaces from 724 faces\n",
      "done in 0.339s\n",
      "Projecting the input data on the eigenfaces orthonormal basis\n",
      "done in 0.073s\n",
      "Fitting the classifier to the training set\n",
      "done in 56.095s\n",
      "Best estimator found by grid search:\n",
      "SVC(C=10, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.002, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "Predicting people's names on the test set\n",
      "done in 0.031s\n",
      "match: 197/242\n"
     ]
    }
   ],
   "source": [
    "##this is the code for the T1 2)\n",
    "from time import time\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "# Display progress logs on stdout\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(message)s')\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "# Download the data, if not already on disk and load it as numpy arrays\n",
    "\n",
    "x = np.load('ex1.npz')\n",
    "X = x['X'] \n",
    "y = x['y']\n",
    "\n",
    "work = np.load('test.npz')\n",
    "work_x = work['X']\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "# Split into a training set and a test set using a stratified k fold\n",
    "\n",
    "# split into a training and testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42)#在生成结果的程序中，训练集占比百分之百\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "# Compute a PCA (eigenfaces) on the face dataset (treated as unlabeled\n",
    "# dataset): unsupervised feature extraction / dimensionality reduction\n",
    "n_components = 150\n",
    "\n",
    "print(\"Extracting the top %d eigenfaces from %d faces\"\n",
    "      % (n_components, X_train.shape[0]))\n",
    "t0 = time()\n",
    "pca = PCA(n_components=n_components, svd_solver='randomized',\n",
    "          whiten=True).fit(X_train)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "\n",
    "\n",
    "print(\"Projecting the input data on the eigenfaces orthonormal basis\")\n",
    "t0 = time()\n",
    "X_train_pca = pca.transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "work_pca = pca.transform(work_x)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "\n",
    "# #############################################################################\n",
    "# Train a SVM classification model\n",
    "\n",
    "print(\"Fitting the classifier to the training set\")\n",
    "t0 = time()\n",
    "param_grid = {'C': [1, 2, 5, 10, 20, 50, 1e2, 500],\n",
    "              'gamma': [0.0001, 0.0005, 0.001, 0.002, 0.005, 0.01, 0.1], }\n",
    "clf = GridSearchCV(SVC(kernel='rbf', class_weight='balanced'),\n",
    "                   param_grid, cv=5, iid=False)\n",
    "clf = clf.fit(X_train_pca, y_train)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print(\"Best estimator found by grid search:\")\n",
    "print(clf.best_estimator_)\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "# Quantitative evaluation of the model quality on the test set\n",
    "\n",
    "print(\"Predicting people's names on the test set\")\n",
    "t0 = time()\n",
    "y_pred = clf.predict(X_test_pca)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "\n",
    "print('match: {0}/{1}'.format(np.sum(np.equal(y_pred,y_test)),y_test.shape[0]))\n",
    "\n",
    "work_y=clf.predict(work_pca)\n",
    "#file_output = open(\"output.txt\",'w',encoding='utf-16')\n",
    "#for i in work_y:\n",
    "    #file_output.write(str(i)+' ')\n",
    "#file_output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T1-2\n",
    "\n",
    "## 说明\n",
    "\n",
    "在生成真正输出数据的时候，我将全部x数据作为训练集。\n",
    "\n",
    "这里的程序展示的是一个训练效果，此时将x划分为1:3的测试/训练集\n",
    "\n",
    "## 结果\n",
    "\n",
    "运行多次发现正确率可以到达83.3％左右，估计output.txt代表的结果可能稍好一点（因为训练集更大）。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
